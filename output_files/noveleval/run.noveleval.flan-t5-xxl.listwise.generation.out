/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]Loading checkpoint shards:  20%|██        | 1/5 [00:01<00:07,  1.96s/it]Loading checkpoint shards:  40%|████      | 2/5 [00:03<00:05,  1.88s/it]Loading checkpoint shards:  60%|██████    | 3/5 [00:05<00:03,  1.92s/it]Loading checkpoint shards:  80%|████████  | 4/5 [00:07<00:01,  1.98s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.73s/it]Loading checkpoint shards: 100%|██████████| 5/5 [00:09<00:00,  1.82s/it]
0it [00:00, ?it/s]24it [00:00, 234.70it/s]54it [00:00, 272.58it/s]89it [00:00, 306.88it/s]164it [00:00, 478.47it/s]212it [00:00, 418.38it/s]255it [00:00, 392.46it/s]296it [00:00, 381.65it/s]485it [00:00, 810.58it/s]572it [00:01, 605.63it/s]691it [00:01, 741.17it/s]777it [00:01, 656.00it/s]960it [00:01, 926.33it/s]1068it [00:01, 815.32it/s]1166it [00:01, 849.23it/s]1261it [00:01, 761.18it/s]1409it [00:02, 929.47it/s]1512it [00:02, 711.13it/s]1598it [00:02, 605.54it/s]1748it [00:02, 781.80it/s]1844it [00:02, 708.90it/s]1969it [00:02, 821.64it/s]2065it [00:02, 746.86it/s]2150it [00:03, 716.06it/s]2229it [00:03, 653.69it/s]2300it [00:03, 547.99it/s]2478it [00:03, 805.96it/s]2574it [00:03, 657.70it/s]2666it [00:03, 708.96it/s]2749it [00:04, 629.11it/s]2847it [00:04, 703.25it/s]2927it [00:04, 629.29it/s]2998it [00:04, 618.37it/s]3065it [00:04, 558.41it/s]3200it [00:04, 739.81it/s]3283it [00:04, 648.24it/s]3388it [00:04, 740.89it/s]3470it [00:05, 639.37it/s]3542it [00:05, 539.76it/s]3789it [00:05, 945.61it/s]3905it [00:05, 763.91it/s]3966it [00:05, 696.37it/s]
  0%|          | 0/21 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  5%|▍         | 1/21 [01:00<20:06, 60.35s/it] 10%|▉         | 2/21 [01:46<16:23, 51.74s/it] 14%|█▍        | 3/21 [02:46<16:39, 55.52s/it] 19%|█▉        | 4/21 [03:46<16:14, 57.30s/it] 24%|██▍       | 5/21 [04:46<15:32, 58.30s/it] 29%|██▊       | 6/21 [05:45<14:38, 58.60s/it] 33%|███▎      | 7/21 [06:43<13:40, 58.59s/it] 38%|███▊      | 8/21 [07:42<12:41, 58.56s/it] 43%|████▎     | 9/21 [08:44<11:55, 59.65s/it] 48%|████▊     | 10/21 [09:47<11:08, 60.74s/it] 52%|█████▏    | 11/21 [10:48<10:08, 60.83s/it] 57%|█████▋    | 12/21 [11:54<09:19, 62.22s/it] 62%|██████▏   | 13/21 [12:55<08:14, 61.84s/it] 67%|██████▋   | 14/21 [13:53<07:06, 60.92s/it] 71%|███████▏  | 15/21 [14:55<06:07, 61.24s/it] 76%|███████▌  | 16/21 [15:55<05:03, 60.62s/it] 81%|████████  | 17/21 [16:54<04:01, 60.39s/it] 86%|████████▌ | 18/21 [17:55<03:01, 60.46s/it] 90%|█████████ | 19/21 [18:58<02:02, 61.08s/it] 95%|█████████▌| 20/21 [19:58<01:00, 60.96s/it]100%|██████████| 21/21 [20:59<00:00, 61.03s/it]100%|██████████| 21/21 [20:59<00:00, 59.99s/it]
Avg comparisons: 242.14285714285714
Avg prompt tokens: 123975.61904761905
Avg completion tokens: 1228.1904761904761
Avg time per query: 59.99489508356367
