/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]57it [00:00, 568.32it/s]1011it [00:00, 5830.57it/s]1595it [00:00, 4148.10it/s]2057it [00:00, 4287.07it/s]3014it [00:00, 5920.12it/s]3651it [00:00, 5176.22it/s]4211it [00:00, 4135.81it/s]5059it [00:01, 5142.24it/s]6019it [00:01, 6231.35it/s]6718it [00:01, 5620.08it/s]7341it [00:01, 5035.39it/s]8046it [00:01, 5491.55it/s]8642it [00:01, 5397.10it/s]9214it [00:01, 4568.95it/s]10048it [00:01, 5438.20it/s]11004it [00:02, 6464.52it/s]11708it [00:02, 5603.74it/s]12326it [00:02, 4645.04it/s]13050it [00:02, 5185.45it/s]14004it [00:02, 6194.30it/s]14693it [00:02, 5405.53it/s]15296it [00:03, 4467.70it/s]16058it [00:03, 5124.40it/s]17019it [00:03, 6163.28it/s]17716it [00:03, 5742.93it/s]18350it [00:03, 5229.24it/s]19050it [00:03, 5612.63it/s]20001it [00:03, 6561.53it/s]20704it [00:03, 5441.57it/s]21309it [00:04, 4498.88it/s]22059it [00:04, 5131.62it/s]23019it [00:04, 6156.24it/s]23713it [00:04, 5688.24it/s]24341it [00:04, 4635.62it/s]25058it [00:04, 5170.17it/s]26014it [00:04, 6197.11it/s]26709it [00:04, 5902.79it/s]27353it [00:05, 5094.84it/s]28056it [00:05, 5529.49it/s]29007it [00:05, 6493.29it/s]29710it [00:05, 5421.06it/s]30316it [00:05, 4449.19it/s]31059it [00:05, 5060.14it/s]32017it [00:05, 6084.35it/s]32706it [00:06, 5737.82it/s]33337it [00:06, 4712.19it/s]34057it [00:06, 5244.25it/s]35011it [00:06, 6251.40it/s]35708it [00:06, 5859.81it/s]36347it [00:06, 5105.24it/s]37054it [00:06, 5535.46it/s]38018it [00:07, 6533.37it/s]38724it [00:07, 5824.66it/s]39354it [00:07, 4839.09it/s]40044it [00:07, 5294.43it/s]41003it [00:07, 6319.76it/s]41698it [00:07, 5317.96it/s]42297it [00:07, 4734.39it/s]43000it [00:07, 5432.49it/s]
  0%|          | 0/43 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  2%|▏         | 1/43 [01:03<44:29, 63.57s/it]  5%|▍         | 2/43 [01:46<35:03, 51.30s/it]  7%|▋         | 3/43 [02:54<39:25, 59.14s/it]  9%|▉         | 4/43 [04:03<40:58, 63.04s/it] 12%|█▏        | 5/43 [04:56<37:37, 59.40s/it] 14%|█▍        | 6/43 [06:07<38:59, 63.24s/it] 16%|█▋        | 7/43 [07:13<38:33, 64.27s/it] 19%|█▊        | 8/43 [08:21<38:09, 65.42s/it] 21%|██        | 9/43 [08:59<32:06, 56.66s/it] 23%|██▎       | 10/43 [10:10<33:40, 61.24s/it] 26%|██▌       | 11/43 [11:00<30:49, 57.78s/it] 28%|██▊       | 12/43 [12:13<32:11, 62.30s/it] 30%|███       | 13/43 [13:00<28:56, 57.88s/it] 33%|███▎      | 14/43 [13:46<26:13, 54.25s/it] 35%|███▍      | 15/43 [14:54<27:12, 58.31s/it] 37%|███▋      | 16/43 [15:34<23:44, 52.76s/it] 40%|███▉      | 17/43 [16:44<25:04, 57.86s/it] 42%|████▏     | 18/43 [17:54<25:41, 61.66s/it] 44%|████▍     | 19/43 [19:14<26:51, 67.16s/it] 47%|████▋     | 20/43 [20:04<23:43, 61.90s/it] 49%|████▉     | 21/43 [21:00<22:07, 60.36s/it] 51%|█████     | 22/43 [21:43<19:15, 55.04s/it] 53%|█████▎    | 23/43 [22:57<20:13, 60.68s/it] 56%|█████▌    | 24/43 [23:57<19:11, 60.60s/it] 58%|█████▊    | 25/43 [24:39<16:29, 54.95s/it] 60%|██████    | 26/43 [25:45<16:29, 58.22s/it] 63%|██████▎   | 27/43 [26:57<16:37, 62.32s/it] 65%|██████▌   | 28/43 [28:07<16:08, 64.54s/it] 67%|██████▋   | 29/43 [28:57<14:04, 60.32s/it] 70%|██████▉   | 30/43 [29:41<11:59, 55.34s/it] 72%|███████▏  | 31/43 [30:22<10:14, 51.23s/it] 74%|███████▍  | 32/43 [31:30<10:18, 56.21s/it] 77%|███████▋  | 33/43 [32:30<09:33, 57.38s/it] 79%|███████▉  | 34/43 [33:17<08:07, 54.15s/it] 81%|████████▏ | 35/43 [34:14<07:20, 55.03s/it] 84%|████████▎ | 36/43 [35:33<07:16, 62.31s/it] 86%|████████▌ | 37/43 [36:40<06:21, 63.65s/it] 88%|████████▊ | 38/43 [37:50<05:27, 65.44s/it] 91%|█████████ | 39/43 [38:52<04:18, 64.64s/it] 93%|█████████▎| 40/43 [39:44<03:01, 60.60s/it] 95%|█████████▌| 41/43 [40:32<01:53, 56.85s/it] 98%|█████████▊| 42/43 [41:25<00:55, 55.63s/it]100%|██████████| 43/43 [42:39<00:00, 61.28s/it]100%|██████████| 43/43 [42:39<00:00, 59.52s/it]
Avg comparisons: 245.0
Avg prompt tokens: 121130.76744186046
Avg completion tokens: 2193.0232558139537
Avg time per query: 59.52248902099077
