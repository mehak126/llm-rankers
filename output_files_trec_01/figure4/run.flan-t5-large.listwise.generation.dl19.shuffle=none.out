/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]53it [00:00, 519.20it/s]1007it [00:00, 5708.15it/s]1578it [00:00, 3989.37it/s]2058it [00:00, 4218.93it/s]3017it [00:00, 5842.08it/s]3650it [00:00, 5221.46it/s]4213it [00:00, 4197.51it/s]5059it [00:01, 5192.01it/s]6021it [00:01, 6289.30it/s]6723it [00:01, 5724.58it/s]7353it [00:01, 5153.58it/s]8046it [00:01, 5565.81it/s]8646it [00:01, 5402.02it/s]9216it [00:01, 4558.06it/s]10049it [00:01, 5419.79it/s]11005it [00:02, 6452.01it/s]11707it [00:02, 5580.44it/s]12322it [00:02, 4619.92it/s]13049it [00:02, 5192.29it/s]14002it [00:02, 6210.27it/s]14695it [00:02, 5439.30it/s]15303it [00:02, 4521.22it/s]16059it [00:03, 5167.85it/s]17020it [00:03, 6185.67it/s]17716it [00:03, 5785.27it/s]18351it [00:03, 5266.25it/s]19050it [00:03, 5648.72it/s]20001it [00:03, 6602.20it/s]20707it [00:03, 5484.29it/s]21316it [00:04, 4518.72it/s]22059it [00:04, 5127.10it/s]23020it [00:04, 6163.43it/s]23714it [00:04, 5789.66it/s]24350it [00:04, 4711.29it/s]25058it [00:04, 5199.60it/s]26014it [00:04, 6228.48it/s]26710it [00:04, 5935.47it/s]27355it [00:05, 5113.13it/s]28056it [00:05, 5538.20it/s]29007it [00:05, 6503.00it/s]29710it [00:05, 5456.13it/s]30318it [00:05, 4466.16it/s]31059it [00:05, 5085.22it/s]32017it [00:05, 6124.67it/s]32711it [00:06, 5787.61it/s]33349it [00:06, 4780.71it/s]34057it [00:06, 5277.09it/s]35011it [00:06, 6288.56it/s]35710it [00:06, 5900.48it/s]36351it [00:06, 5178.52it/s]37054it [00:06, 5583.72it/s]38018it [00:06, 6572.90it/s]38725it [00:07, 5874.18it/s]39358it [00:07, 4893.22it/s]40044it [00:07, 5326.52it/s]41003it [00:07, 6329.76it/s]41695it [00:07, 5317.32it/s]42291it [00:07, 4718.56it/s]43000it [00:07, 5457.40it/s]
  0%|          | 0/43 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  2%|▏         | 1/43 [01:11<50:02, 71.49s/it]  5%|▍         | 2/43 [01:52<36:41, 53.69s/it]  7%|▋         | 3/43 [03:00<40:10, 60.27s/it]  9%|▉         | 4/43 [04:15<42:53, 65.99s/it] 12%|█▏        | 5/43 [05:09<38:58, 61.54s/it] 14%|█▍        | 6/43 [06:22<40:30, 65.69s/it] 16%|█▋        | 7/43 [07:23<38:27, 64.09s/it] 19%|█▊        | 8/43 [08:42<40:09, 68.85s/it] 21%|██        | 9/43 [09:23<34:02, 60.06s/it] 23%|██▎       | 10/43 [10:33<34:41, 63.08s/it] 26%|██▌       | 11/43 [11:26<32:00, 60.02s/it] 28%|██▊       | 12/43 [12:35<32:24, 62.73s/it] 30%|███       | 13/43 [13:29<30:07, 60.24s/it] 33%|███▎      | 14/43 [14:23<28:07, 58.18s/it] 35%|███▍      | 15/43 [15:36<29:13, 62.63s/it] 37%|███▋      | 16/43 [16:18<25:25, 56.48s/it] 40%|███▉      | 17/43 [17:31<26:41, 61.58s/it] 42%|████▏     | 18/43 [18:37<26:06, 62.67s/it] 44%|████▍     | 19/43 [19:58<27:17, 68.25s/it] 47%|████▋     | 20/43 [20:46<23:49, 62.15s/it] 49%|████▉     | 21/43 [21:51<23:09, 63.14s/it] 51%|█████     | 22/43 [22:38<20:22, 58.20s/it] 53%|█████▎    | 23/43 [23:53<21:03, 63.16s/it] 56%|█████▌    | 24/43 [24:55<19:54, 62.87s/it] 58%|█████▊    | 25/43 [25:38<17:07, 57.06s/it] 60%|██████    | 26/43 [26:49<17:21, 61.28s/it] 63%|██████▎   | 27/43 [28:02<17:16, 64.79s/it] 65%|██████▌   | 28/43 [29:09<16:20, 65.34s/it] 67%|██████▋   | 29/43 [30:00<14:15, 61.09s/it] 70%|██████▉   | 30/43 [30:49<12:24, 57.24s/it] 72%|███████▏  | 31/43 [31:36<10:50, 54.21s/it] 74%|███████▍  | 32/43 [32:49<10:58, 59.90s/it] 77%|███████▋  | 33/43 [33:53<10:10, 61.10s/it] 79%|███████▉  | 34/43 [34:37<08:23, 55.92s/it] 81%|████████▏ | 35/43 [35:35<07:33, 56.65s/it] 84%|████████▎ | 36/43 [36:56<07:27, 63.88s/it] 86%|████████▌ | 37/43 [38:01<06:26, 64.43s/it] 88%|████████▊ | 38/43 [39:14<05:33, 66.74s/it] 91%|█████████ | 39/43 [40:19<04:25, 66.30s/it] 93%|█████████▎| 40/43 [41:14<03:08, 62.94s/it] 95%|█████████▌| 41/43 [42:14<02:04, 62.05s/it] 98%|█████████▊| 42/43 [43:06<00:59, 59.17s/it]100%|██████████| 43/43 [44:26<00:00, 65.38s/it]100%|██████████| 43/43 [44:26<00:00, 62.02s/it]
Avg comparisons: 245.0
Avg prompt tokens: 120653.81395348837
Avg completion tokens: 2206.8372093023254
Avg time per query: 62.01580222817354
