/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]32it [00:00, 314.36it/s]70it [00:00, 351.69it/s]151it [00:00, 555.13it/s]207it [00:00, 488.56it/s]257it [00:00, 459.16it/s]304it [00:00, 450.17it/s]498it [00:00, 891.60it/s]592it [00:00, 765.51it/s]698it [00:01, 839.26it/s]788it [00:01, 798.35it/s]969it [00:01, 1064.17it/s]1082it [00:01, 1006.60it/s]1188it [00:01, 938.85it/s] 1378it [00:01, 1186.04it/s]1503it [00:01, 850.10it/s] 1606it [00:02, 746.91it/s]1754it [00:02, 892.27it/s]1859it [00:02, 850.53it/s]1975it [00:02, 920.28it/s]2077it [00:02, 894.11it/s]2174it [00:02, 783.65it/s]2259it [00:02, 731.05it/s]2337it [00:02, 690.80it/s]2506it [00:03, 925.92it/s]2607it [00:03, 903.22it/s]2703it [00:03, 753.82it/s]2833it [00:03, 876.67it/s]2930it [00:03, 767.80it/s]3015it [00:03, 729.32it/s]3168it [00:03, 915.96it/s]3269it [00:04, 734.46it/s]3396it [00:04, 851.03it/s]3494it [00:04, 735.82it/s]3579it [00:04, 676.11it/s]3822it [00:04, 1057.92it/s]3947it [00:04, 1066.12it/s]3966it [00:04, 835.11it/s] 
  0%|          | 0/21 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  5%|▍         | 1/21 [00:33<11:00, 33.04s/it] 10%|▉         | 2/21 [00:57<08:54, 28.11s/it] 14%|█▍        | 3/21 [01:30<09:03, 30.20s/it] 19%|█▉        | 4/21 [02:04<08:57, 31.60s/it] 24%|██▍       | 5/21 [02:39<08:49, 33.07s/it] 29%|██▊       | 6/21 [03:13<08:21, 33.40s/it] 33%|███▎      | 7/21 [03:46<07:44, 33.15s/it] 38%|███▊      | 8/21 [04:18<07:07, 32.90s/it] 43%|████▎     | 9/21 [04:51<06:32, 32.72s/it] 48%|████▊     | 10/21 [05:25<06:06, 33.35s/it] 52%|█████▏    | 11/21 [05:58<05:30, 33.01s/it] 57%|█████▋    | 12/21 [06:32<05:01, 33.51s/it] 62%|██████▏   | 13/21 [07:05<04:24, 33.11s/it] 67%|██████▋   | 14/21 [07:37<03:50, 32.86s/it] 71%|███████▏  | 15/21 [08:11<03:20, 33.39s/it] 76%|███████▌  | 16/21 [08:44<02:45, 33.04s/it] 81%|████████  | 17/21 [09:16<02:11, 32.78s/it] 86%|████████▌ | 18/21 [09:48<01:37, 32.61s/it] 90%|█████████ | 19/21 [10:20<01:04, 32.48s/it] 95%|█████████▌| 20/21 [10:52<00:32, 32.38s/it]100%|██████████| 21/21 [11:25<00:00, 32.32s/it]100%|██████████| 21/21 [11:25<00:00, 32.62s/it]
Avg comparisons: 242.14285714285714
Avg prompt tokens: 123971.14285714286
Avg completion tokens: 1230.7619047619048
Avg time per query: 32.6209705216544
