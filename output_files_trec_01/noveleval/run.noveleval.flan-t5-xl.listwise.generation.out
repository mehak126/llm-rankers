/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.98s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  2.67s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it]
0it [00:00, ?it/s]31it [00:00, 303.03it/s]69it [00:00, 344.45it/s]151it [00:00, 554.21it/s]207it [00:00, 487.79it/s]257it [00:00, 456.79it/s]304it [00:00, 446.57it/s]499it [00:00, 891.12it/s]594it [00:00, 767.86it/s]699it [00:01, 839.21it/s]789it [00:01, 799.74it/s]968it [00:01, 1060.97it/s]1081it [00:01, 1004.63it/s]1187it [00:01, 940.54it/s] 1377it [00:01, 1185.52it/s]1502it [00:01, 847.12it/s] 1604it [00:02, 744.46it/s]1756it [00:02, 902.89it/s]1863it [00:02, 890.24it/s]1975it [00:02, 941.45it/s]2079it [00:02, 909.63it/s]2177it [00:02, 778.51it/s]2262it [00:02, 721.75it/s]2340it [00:02, 687.56it/s]2500it [00:03, 900.45it/s]2598it [00:03, 846.63it/s]2689it [00:03, 787.76it/s]2818it [00:03, 910.69it/s]2915it [00:03, 729.71it/s]3000it [00:03, 754.07it/s]3083it [00:03, 733.97it/s]3207it [00:03, 856.31it/s]3299it [00:04, 815.88it/s]3396it [00:04, 852.70it/s]3485it [00:04, 735.67it/s]3564it [00:04, 630.86it/s]3820it [00:04, 1070.85it/s]3945it [00:04, 1061.08it/s]3966it [00:04, 832.77it/s] 
  0%|          | 0/21 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  5%|▍         | 1/21 [00:42<14:14, 42.75s/it] 10%|▉         | 2/21 [01:13<11:15, 35.58s/it] 14%|█▍        | 3/21 [01:49<10:42, 35.68s/it] 19%|█▉        | 4/21 [02:24<10:06, 35.66s/it] 24%|██▍       | 5/21 [03:02<09:41, 36.35s/it] 29%|██▊       | 6/21 [03:39<09:09, 36.65s/it] 33%|███▎      | 7/21 [04:15<08:28, 36.33s/it] 38%|███▊      | 8/21 [04:48<07:40, 35.42s/it] 43%|████▎     | 9/21 [05:24<07:05, 35.46s/it] 48%|████▊     | 10/21 [06:01<06:36, 36.00s/it] 52%|█████▏    | 11/21 [06:37<05:59, 35.91s/it] 57%|█████▋    | 12/21 [07:17<05:34, 37.15s/it] 62%|██████▏   | 13/21 [07:58<05:08, 38.51s/it] 67%|██████▋   | 14/21 [08:37<04:29, 38.52s/it] 71%|███████▏  | 15/21 [09:17<03:54, 39.08s/it] 76%|███████▌  | 16/21 [09:53<03:09, 37.96s/it] 81%|████████  | 17/21 [10:28<02:28, 37.19s/it] 86%|████████▌ | 18/21 [11:03<01:49, 36.62s/it] 90%|█████████ | 19/21 [11:39<01:12, 36.24s/it] 95%|█████████▌| 20/21 [12:14<00:35, 35.98s/it]100%|██████████| 21/21 [12:49<00:00, 35.79s/it]100%|██████████| 21/21 [12:49<00:00, 36.66s/it]
Avg comparisons: 242.14285714285714
Avg prompt tokens: 123975.42857142857
Avg completion tokens: 1227.7619047619048
Avg time per query: 36.658036220641364
