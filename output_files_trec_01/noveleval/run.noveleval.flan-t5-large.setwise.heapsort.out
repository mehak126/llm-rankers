/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]27it [00:00, 264.75it/s]60it [00:00, 299.40it/s]94it [00:00, 315.83it/s]170it [00:00, 487.15it/s]219it [00:00, 427.73it/s]263it [00:00, 401.38it/s]304it [00:00, 388.17it/s]493it [00:00, 814.04it/s]580it [00:01, 634.15it/s]692it [00:01, 748.39it/s]777it [00:01, 660.61it/s]961it [00:01, 939.01it/s]1069it [00:01, 831.42it/s]1166it [00:01, 863.18it/s]1261it [00:01, 769.88it/s]1410it [00:01, 935.99it/s]1513it [00:02, 709.69it/s]1598it [00:02, 618.55it/s]1750it [00:02, 795.81it/s]1846it [00:02, 720.52it/s]1968it [00:02, 825.05it/s]2063it [00:02, 732.50it/s]2146it [00:03, 711.88it/s]2224it [00:03, 653.69it/s]2295it [00:03, 557.26it/s]2475it [00:03, 820.37it/s]2571it [00:03, 659.79it/s]2660it [00:03, 705.61it/s]2743it [00:04, 630.19it/s]2847it [00:04, 718.18it/s]2929it [00:04, 635.68it/s]3001it [00:04, 595.27it/s]3066it [00:04, 553.19it/s]3200it [00:04, 731.63it/s]3282it [00:04, 642.85it/s]3389it [00:04, 736.88it/s]3471it [00:05, 634.29it/s]3542it [00:05, 541.27it/s]3790it [00:05, 952.92it/s]3907it [00:05, 782.64it/s]3966it [00:05, 701.50it/s]
  0%|          | 0/21 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors
  5%|▍         | 1/21 [00:05<01:58,  5.91s/it] 10%|▉         | 2/21 [00:11<01:44,  5.49s/it] 14%|█▍        | 3/21 [00:17<01:42,  5.68s/it] 19%|█▉        | 4/21 [00:22<01:35,  5.60s/it] 24%|██▍       | 5/21 [00:28<01:31,  5.70s/it] 29%|██▊       | 6/21 [00:34<01:27,  5.85s/it] 33%|███▎      | 7/21 [00:40<01:23,  5.94s/it] 38%|███▊      | 8/21 [00:46<01:16,  5.90s/it] 43%|████▎     | 9/21 [00:51<01:09,  5.76s/it] 48%|████▊     | 10/21 [00:57<01:02,  5.70s/it] 52%|█████▏    | 11/21 [01:03<00:56,  5.69s/it] 57%|█████▋    | 12/21 [01:08<00:51,  5.67s/it] 62%|██████▏   | 13/21 [01:14<00:45,  5.69s/it] 67%|██████▋   | 14/21 [01:20<00:39,  5.65s/it] 71%|███████▏  | 15/21 [01:26<00:34,  5.76s/it] 76%|███████▌  | 16/21 [01:32<00:29,  5.86s/it] 81%|████████  | 17/21 [01:37<00:23,  5.84s/it] 86%|████████▌ | 18/21 [01:43<00:17,  5.83s/it] 90%|█████████ | 19/21 [01:49<00:11,  5.87s/it] 95%|█████████▌| 20/21 [01:55<00:05,  5.80s/it]100%|██████████| 21/21 [02:01<00:00,  5.80s/it]100%|██████████| 21/21 [02:01<00:00,  5.77s/it]
Avg comparisons: 72.38095238095238
Avg prompt tokens: 41609.19047619047
Avg completion tokens: 361.9047619047619
Avg time per query: 5.769212359473819
