/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]27it [00:00, 267.21it/s]59it [00:00, 293.55it/s]93it [00:00, 313.97it/s]168it [00:00, 484.09it/s]217it [00:00, 425.02it/s]261it [00:00, 400.07it/s]302it [00:00, 384.39it/s]491it [00:00, 809.58it/s]578it [00:01, 623.96it/s]692it [00:01, 743.68it/s]777it [00:01, 658.01it/s]961it [00:01, 935.70it/s]1069it [00:01, 828.22it/s]1166it [00:01, 860.91it/s]1261it [00:01, 766.34it/s]1409it [00:02, 932.79it/s]1512it [00:02, 708.63it/s]1597it [00:02, 614.44it/s]1750it [00:02, 793.72it/s]1846it [00:02, 718.98it/s]1966it [00:02, 821.91it/s]2061it [00:02, 721.60it/s]2144it [00:03, 710.35it/s]2222it [00:03, 652.61it/s]2293it [00:03, 554.36it/s]2473it [00:03, 816.58it/s]2569it [00:03, 649.54it/s]2666it [00:03, 710.52it/s]2750it [00:04, 655.61it/s]2846it [00:04, 719.20it/s]2927it [00:04, 610.86it/s]2997it [00:04, 611.73it/s]3064it [00:04, 555.23it/s]3200it [00:04, 738.73it/s]3283it [00:04, 649.57it/s]3389it [00:04, 740.48it/s]3471it [00:05, 636.65it/s]3543it [00:05, 542.71it/s]3791it [00:05, 954.15it/s]3909it [00:05, 788.58it/s]3966it [00:05, 700.81it/s]
  0%|          | 0/21 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (589 > 512). Running this sequence through the model will result in indexing errors
  5%|▍         | 1/21 [00:25<08:25, 25.28s/it] 10%|▉         | 2/21 [00:44<06:52, 21.68s/it] 14%|█▍        | 3/21 [01:15<07:50, 26.13s/it] 19%|█▉        | 4/21 [01:43<07:35, 26.81s/it] 24%|██▍       | 5/21 [02:17<07:46, 29.17s/it] 29%|██▊       | 6/21 [02:49<07:32, 30.19s/it] 33%|███▎      | 7/21 [03:21<07:11, 30.82s/it] 38%|███▊      | 8/21 [03:54<06:48, 31.41s/it] 43%|████▎     | 9/21 [04:25<06:16, 31.39s/it] 48%|████▊     | 10/21 [04:58<05:51, 31.99s/it] 52%|█████▏    | 11/21 [05:32<05:24, 32.43s/it] 57%|█████▋    | 12/21 [06:05<04:53, 32.63s/it] 62%|██████▏   | 13/21 [06:36<04:16, 32.11s/it] 67%|██████▋   | 14/21 [07:03<03:35, 30.75s/it] 71%|███████▏  | 15/21 [07:37<03:09, 31.51s/it] 76%|███████▌  | 16/21 [08:08<02:37, 31.53s/it] 81%|████████  | 17/21 [08:41<02:07, 31.82s/it] 86%|████████▌ | 18/21 [09:13<01:35, 31.94s/it] 90%|█████████ | 19/21 [09:43<01:02, 31.32s/it] 95%|█████████▌| 20/21 [10:05<00:28, 28.52s/it]100%|██████████| 21/21 [10:30<00:00, 27.46s/it]100%|██████████| 21/21 [10:30<00:00, 30.01s/it]
Avg comparisons: 306.7142857142857
Avg prompt tokens: 175563.7142857143
Avg completion tokens: 1533.5714285714287
Avg time per query: 30.00886557215736
