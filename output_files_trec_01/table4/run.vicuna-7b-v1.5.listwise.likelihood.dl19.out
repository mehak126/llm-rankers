huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.56s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]
/home/mdhaliwal/miniconda3/envs/test-llmrankers/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/mdhaliwal/miniconda3/envs/test-llmrankers/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/home/mdhaliwal/miniconda3/envs/test-llmrankers/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/home/mdhaliwal/miniconda3/envs/test-llmrankers/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
0it [00:00, ?it/s]2083it [00:00, 20774.55it/s]5072it [00:00, 26096.01it/s]8074it [00:00, 27848.65it/s]11078it [00:00, 28711.21it/s]14074it [00:00, 29134.87it/s]17086it [00:00, 29459.36it/s]20085it [00:00, 29619.28it/s]23061it [00:00, 29476.00it/s]26053it [00:00, 29610.21it/s]29051it [00:01, 29680.37it/s]32045it [00:01, 29741.55it/s]35043it [00:01, 29799.58it/s]38047it [00:01, 29869.90it/s]41035it [00:01, 29347.70it/s]43000it [00:01, 29304.13it/s]
  0%|          | 0/43 [00:00<?, ?it/s]  2%|▏         | 1/43 [00:11<07:50, 11.19s/it]  5%|▍         | 2/43 [00:22<07:52, 11.52s/it]  7%|▋         | 3/43 [00:34<07:39, 11.48s/it]  9%|▉         | 4/43 [00:46<07:31, 11.58s/it] 12%|█▏        | 5/43 [00:57<07:23, 11.68s/it] 14%|█▍        | 6/43 [01:08<06:59, 11.34s/it] 16%|█▋        | 7/43 [01:19<06:46, 11.30s/it] 19%|█▊        | 8/43 [01:30<06:31, 11.19s/it] 21%|██        | 9/43 [01:42<06:30, 11.49s/it] 23%|██▎       | 10/43 [01:53<06:13, 11.33s/it] 26%|██▌       | 11/43 [02:05<06:08, 11.52s/it] 28%|██▊       | 12/43 [02:16<05:52, 11.37s/it] 30%|███       | 13/43 [02:28<05:45, 11.51s/it] 33%|███▎      | 14/43 [02:40<05:35, 11.59s/it] 35%|███▍      | 15/43 [02:51<05:22, 11.52s/it] 37%|███▋      | 16/43 [03:03<05:10, 11.50s/it] 40%|███▉      | 17/43 [03:14<04:57, 11.45s/it] 42%|████▏     | 18/43 [03:25<04:41, 11.26s/it] 44%|████▍     | 19/43 [03:35<04:24, 11.04s/it] 47%|████▋     | 20/43 [03:47<04:20, 11.32s/it] 49%|████▉     | 21/43 [03:58<04:06, 11.23s/it] 51%|█████     | 22/43 [04:11<04:01, 11.49s/it] 53%|█████▎    | 23/43 [04:21<03:43, 11.19s/it] 56%|█████▌    | 24/43 [04:32<03:32, 11.19s/it] 58%|█████▊    | 25/43 [04:44<03:25, 11.43s/it] 60%|██████    | 26/43 [04:56<03:13, 11.40s/it] 63%|██████▎   | 27/43 [05:06<02:57, 11.08s/it] 65%|██████▌   | 28/43 [05:17<02:45, 11.05s/it] 67%|██████▋   | 29/43 [05:28<02:36, 11.15s/it] 70%|██████▉   | 30/43 [05:40<02:27, 11.34s/it] 72%|███████▏  | 31/43 [05:52<02:18, 11.52s/it] 74%|███████▍  | 32/43 [06:03<02:05, 11.40s/it] 77%|███████▋  | 33/43 [06:14<01:53, 11.38s/it] 79%|███████▉  | 34/43 [06:26<01:43, 11.45s/it] 81%|████████▏ | 35/43 [06:38<01:31, 11.47s/it] 84%|████████▎ | 36/43 [06:49<01:19, 11.31s/it] 86%|████████▌ | 37/43 [07:00<01:07, 11.22s/it] 88%|████████▊ | 38/43 [07:11<00:55, 11.16s/it] 91%|█████████ | 39/43 [07:22<00:44, 11.17s/it] 93%|█████████▎| 40/43 [07:34<00:34, 11.42s/it] 95%|█████████▌| 41/43 [07:45<00:22, 11.29s/it] 98%|█████████▊| 42/43 [07:56<00:11, 11.42s/it]100%|██████████| 43/43 [08:08<00:00, 11.46s/it]100%|██████████| 43/43 [08:08<00:00, 11.36s/it]
Avg comparisons: 245.0
Avg prompt tokens: 98890.86046511628
Avg completion tokens: 0.0
Avg time per query: 11.361046075820923
