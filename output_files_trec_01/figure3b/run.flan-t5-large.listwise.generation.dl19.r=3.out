/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565
Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at google/flan-t5-large and are newly initialized: ['decoder.embed_tokens.weight', 'encoder.embed_tokens.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
0it [00:00, ?it/s]60it [00:00, 590.33it/s]1019it [00:00, 5800.83it/s]1600it [00:00, 4661.82it/s]2088it [00:00, 4083.46it/s]3050it [00:00, 5707.71it/s]4006it [00:00, 6826.07it/s]4734it [00:00, 5765.65it/s]5364it [00:01, 5078.09it/s]6055it [00:01, 5500.17it/s]7015it [00:01, 6537.60it/s]7719it [00:01, 6266.44it/s]8381it [00:01, 5093.70it/s]9062it [00:01, 5481.84it/s]10016it [00:01, 6478.11it/s]10720it [00:01, 5947.03it/s]11360it [00:02, 5174.62it/s]12055it [00:02, 5573.22it/s]13012it [00:02, 6550.09it/s]13715it [00:02, 5838.12it/s]14343it [00:02, 5107.25it/s]15055it [00:02, 5570.03it/s]16012it [00:02, 6559.65it/s]16718it [00:02, 6040.44it/s]17362it [00:03, 5306.07it/s]18066it [00:03, 5709.59it/s]19031it [00:03, 6704.60it/s]19747it [00:03, 6369.24it/s]20417it [00:03, 5434.32it/s]21055it [00:03, 5657.79it/s]22011it [00:03, 6642.36it/s]22718it [00:03, 6151.09it/s]23368it [00:04, 5376.04it/s]24054it [00:04, 5720.55it/s]25009it [00:04, 6689.14it/s]25719it [00:04, 6030.89it/s]26360it [00:04, 5414.39it/s]27061it [00:04, 5792.78it/s]28020it [00:04, 6733.45it/s]28731it [00:04, 6338.92it/s]29394it [00:05, 5324.51it/s]30057it [00:05, 5612.83it/s]31010it [00:05, 6599.52it/s]31715it [00:05, 6007.30it/s]32355it [00:05, 5291.06it/s]33055it [00:05, 5684.36it/s]34012it [00:05, 6656.89it/s]34721it [00:05, 6015.95it/s]35362it [00:06, 5426.82it/s]36067it [00:06, 5805.59it/s]37026it [00:06, 6762.65it/s]37741it [00:06, 6532.51it/s]38422it [00:06, 5609.84it/s]39059it [00:06, 5780.94it/s]40012it [00:06, 6740.22it/s]40723it [00:06, 6008.42it/s]41361it [00:07, 5150.17it/s]42062it [00:07, 5563.48it/s]43000it [00:07, 5896.05it/s]
  0%|          | 0/43 [00:00<?, ?it/s]/home/mdhaliwal/miniconda3/envs/llmrankers/lib/python3.9/site-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.
  warnings.warn(
  2%|▏         | 1/43 [00:46<32:28, 46.38s/it]  5%|▍         | 2/43 [01:18<26:01, 38.09s/it]  7%|▋         | 3/43 [02:04<27:50, 41.76s/it]  9%|▉         | 4/43 [02:52<28:45, 44.25s/it] 12%|█▏        | 5/43 [03:34<27:20, 43.18s/it] 14%|█▍        | 6/43 [04:22<27:40, 44.87s/it] 16%|█▋        | 7/43 [05:07<26:58, 44.95s/it] 19%|█▊        | 8/43 [05:58<27:19, 46.83s/it] 21%|██        | 9/43 [06:31<24:08, 42.60s/it] 23%|██▎       | 10/43 [07:19<24:18, 44.19s/it] 26%|██▌       | 11/43 [07:58<22:46, 42.70s/it] 28%|██▊       | 12/43 [08:46<22:53, 44.32s/it] 30%|███       | 13/43 [09:25<21:23, 42.78s/it] 33%|███▎      | 14/43 [10:07<20:27, 42.34s/it] 35%|███▍      | 15/43 [10:55<20:33, 44.04s/it] 37%|███▋      | 16/43 [11:26<18:07, 40.28s/it] 40%|███▉      | 17/43 [12:13<18:17, 42.20s/it] 42%|████▏     | 18/43 [12:58<17:53, 42.93s/it] 44%|████▍     | 19/43 [13:50<18:17, 45.74s/it] 47%|████▋     | 20/43 [14:30<16:52, 44.04s/it] 49%|████▉     | 21/43 [15:18<16:38, 45.39s/it] 51%|█████     | 22/43 [15:51<14:33, 41.61s/it] 53%|█████▎    | 23/43 [16:39<14:28, 43.43s/it] 56%|█████▌    | 24/43 [17:20<13:29, 42.63s/it] 58%|█████▊    | 25/43 [17:56<12:13, 40.74s/it] 60%|██████    | 26/43 [18:43<12:04, 42.64s/it] 63%|██████▎   | 27/43 [19:25<11:19, 42.50s/it] 65%|██████▌   | 28/43 [20:10<10:45, 43.04s/it] 67%|██████▋   | 29/43 [20:45<09:29, 40.70s/it] 70%|██████▉   | 30/43 [21:19<08:22, 38.64s/it] 72%|███████▏  | 31/43 [21:52<07:25, 37.15s/it] 74%|███████▍  | 32/43 [22:39<07:18, 39.90s/it] 77%|███████▋  | 33/43 [23:20<06:43, 40.40s/it] 79%|███████▉  | 34/43 [23:56<05:50, 38.96s/it] 81%|████████▏ | 35/43 [24:35<05:11, 38.89s/it] 84%|████████▎ | 36/43 [25:23<04:52, 41.72s/it] 86%|████████▌ | 37/43 [26:06<04:13, 42.28s/it] 88%|████████▊ | 38/43 [26:55<03:40, 44.16s/it] 91%|█████████ | 39/43 [27:39<02:56, 44.24s/it] 93%|█████████▎| 40/43 [28:24<02:13, 44.40s/it] 95%|█████████▌| 41/43 [29:03<01:25, 42.62s/it] 98%|█████████▊| 42/43 [29:49<00:43, 43.71s/it]100%|██████████| 43/43 [30:38<00:00, 45.28s/it]100%|██████████| 43/43 [30:38<00:00, 42.75s/it]
Avg comparisons: 147.0
Avg prompt tokens: 71425.76744186046
Avg completion tokens: 1534.0232558139535
Avg time per query: 42.751561880111694
